# üìù 0_PLAN_ETL_CENTRAL.md

---

## üéØ Objetivo General
Centralizar el procesamiento de datos crudos de Spider y GOSOM, consolid√°ndolos en un "CSV Madre" (conceptual, en memoria/descargable) y generando chunks si es necesario, utilizando una aplicaci√≥n Next.js/React. Se contempla la futura carga a PostgreSQL.

---

## (FASE 2 BASE DE DATOS)


---
## üß∞ M√≥dulos y Tareas a Desarrollar (Adaptado a Next.js/React)

### 1. **L√≥gica Central (TypeScript en `src/app/(components)/data-refinery/`, `src/lib/etl-logic.ts` o Genkit Flows)**
- [ ] **Consolidaci√≥n de Datos (`consolidateData`)**
  - [ ] Parsear CSVs de Spider y Gosom (usar una librer√≠a como PapaParse).
  - [ ] L√≥gica de merge de datos.
  - [ ] Manejar campos diferentes entre Spider y Gosom (ej: `url` vs `website`), priorizando Gosom en conflictos seg√∫n `etl_params.json`.
  - [ ] Implementar deduplicaci√≥n real (basado en `deduplication_keys` de `etl_params.json`).
 - _Actual: Simulado en `ConsolidateTabContent.tsx`._
- [ ] **Generaci√≥n de Chunks (`generateChunks`)**
  - [ ] Dividir datos consolidados seg√∫n `chunkSize`.
  - [ ] Preparar cada chunk para descarga como CSV.
  - [ ] Implementar l√≥gica para seleccionar columnas a incluir en los chunks.
  - [ ] A√±adir columnas `id_chunk` y `fecha_chunk` a cada registro dentro de los chunks generados.
- [ ] **Registro de Logs**
  - [x] Implementado mediante `addLog` prop en `page.tsx` y mostrado en `LogsTabContent.tsx`.
  - _Futuro: Si se usa backend/Genkit, considerar logging estructurado all√≠._
### 2. **Interfaz de Procesamiento (`src/app/page.tsx` y componentes asociados en `src/app/(components)/data-refinery/`)**
- [x] **Sidebar:**
  - [x] `Input` (ShadCN) para `chunkSize` (controlado en `page.tsx`, con min/max de `etl_params.json`).
- [x] ‚úÖ **√Årea Principal con Pesta√±as (ShadCN `Tabs`):**
  - [x] ‚úÖ **Cargar CSVs (`UploadTabContent.tsx`):**
    - [x] ‚úÖ Inputs de carga de archivos separados para Spider y Gosom.
    - [ ] Validaci√≥n real de campos requeridos (definidos en `upload_validation` de `etl_params.json`) al cargar. _Actual: Validaci√≥n de tipo .csv simulada._
  - [x] ‚úÖ **Consolidar (`ConsolidateTabContent.tsx`):**
    - [x] ‚úÖ Bot√≥n para iniciar la consolidaci√≥n.‚úÖ Mostrar datos consolidados reales (post-procesamiento).‚úÖ Permitir descarga del CSV Madre consolidado.
  - [x] ‚úÖ **Chunkear (`ChunkingTabContent.tsx`):**
    - [ ] Reemplazar la vista previa b√°sica con `JSON.stringify` por una tabla interactiva para visualizar los datos consolidados.
    - [x] ‚úÖ Bot√≥n para generar chunks.‚úÖ Usar datos consolidados reales.
    - [x] ‚úÖ Mostrar lista de chunks reales generados para descarga. _Actual: Simulado._
    - [ ] A√±adir interfaz de usuario para seleccionar las columnas deseadas para el chunking.
  - [x] ‚úÖ **Logs (`LogsTabContent.tsx`):**
    - [x] ‚úÖ Muestra logs de la UI y operaciones simuladas.
 
### 3. **Configuraci√≥n y Datos (Client-Side / Conceptual)**
- [x] `config/etl_params.json`:
    - [x] ‚úÖ `chunk_size_default`, `chunk_size_min`, `chunk_size_max`.
    - [x] ‚úÖ `upload_validation` (campos requeridos para Spider/Gosom).
 - [ ] Refinar `upload_validation`, `deduplication_keys` y `conflict_resolution_priority_source` bas√°ndose en las columnas reales de los archivos de origen (Spider y Gosom de ejemplo).
    - [x] ‚úÖ `deduplication_keys`.
    - [x] ‚úÖ `conflict_resolution_priority_source`.
    - [x] ‚úÖ `postgresql_config_placeholder`.
    - [x] ‚úÖ `log_file_path_conceptual` (aclarando que los logs son en UI para la app cliente).
- **Archivos de Datos (Manejo en el Navegador):**
  - **CSVs Crudos (Spider/Gosom):** Cargados por el usuario v√≠a `<input type="file">`, procesados en memoria. No se guardan en `data/raw/` en el cliente.
  - **CSV Madre (Consolidado):** Estado en la aplicaci√≥n React (`consolidatedData` en `page.tsx`), descargable. No se guarda en `data/consolidated/` en el cliente.
  - **Chunks Generados:** Objetos en memoria, descargables individualmente. No se guardan en `data/chunks/` en el cliente.
  - **Logs de Ejecuci√≥n:** Array en el estado de la app (`logs` en `page.tsx`), mostrados en la UI. No se escribe a `data/logs/etl_central.log` en el cliente.

### 4. **Pruebas & CI/CD (Potencial Futuro)**
- [ ] GitHub Actions: configurar para ejecutar pruebas automatizadas.

---

## üìö Historial y Checklist de Desarrollo
- **Fase 1: MVP (Actual - Funcionalidad Simulada)**
  - [x] ‚úÖ Interfaz b√°sica con carga de archivos (validaci√≥n de tipo de archivo).
  - [x] ‚úÖ Consolidaci√≥n simulada, deduplicaci√≥n simulada, chunking simulado.
  - [x] ‚úÖ Interfaz completamente funcional con Next.js/React y ShadCN UI.
  - [x] ‚úÖ Configuraci√≥n y uso b√°sico de `etl_params.json` para `chunkSize`.
  - [x] Implementar L√≥gica Real de Parseo CSV.
  - [x] Implementar L√≥gica Real de Consolidaci√≥n.
  - [x] Implementar L√≥gica Real de Deduplicaci√≥n.
  - [x] Implementar L√≥gica Real de Chunking.
  - [x] ‚úÖ Sistema de logging en la UI.
- **Fase 2: Implementaci√≥n de L√≥gica Real de ETL (Client-Side)**
  - [ ] Implementar parseo real de CSV (ej: PapaParse) en `UploadTabContent.tsx` o `src/lib/etl-logic.ts`.
  - [ ] Mejorar manejo de errores y feedback al usuario en todas las pesta√±as.
  - [ ] Asegurar que la descarga de CSVs (consolidado y chunks) funcione con datos reales.
- **Fase 3: Integraci√≥n con Backend (Potencial - PostgreSQL)**
  - [ ] Identificar operaciones que se beneficiar√≠an de un backend (ej: procesamiento de archivos muy grandes, persistencia).
  - [ ] Desarrollar Genkit flows para dichas operaciones si es necesario.
  - [ ] Implementar carga a base de datos PostgreSQL (requiere Genkit flow o API de backend, usando `postgresql_config_placeholder` de `etl_params.json`).
- **Fase 4: Automatizaci√≥n (Requerir√≠a Backend/Servicios Cloud)**
 - [ ] Considerar ejecuci√≥n autom√°tica/programada si se migra a una arquitectura con backend.
1. **Validar este `0_PLAN_ETL_CENTRAL.md` actualizado.** (Iteraci√≥n actual).
2. **Implementar parseo y validaci√≥n de CSVs en `UploadTabContent.tsx`:**
    - Utilizar una librer√≠a como PapaParse para leer el contenido de los archivos CSV.
    - Leer `upload_validation` de `config/etl_params.json`.
    - Validar que los encabezados requeridos existan en los respectivos archivos.
    - Mostrar errores espec√≠ficos al usuario en la UI (`toast` y/o mensajes en la Card) si la validaci√≥n falla.
    - Si la validaci√≥n es exitosa, almacenar los datos parseados (array de objetos) en el estado (`spiderFileData`, `gosomFileData`) en lugar de solo el objeto `File`.
3. **Implementar la l√≥gica real de consolidaci√≥n y deduplicaci√≥n:**
    - Crear/Usar un archivo para la l√≥gica, ej: `src/lib/etl-logic.ts`.
    - Crear funciones como:
        - `function consolidateAndDeduplicate(spiderData: Record<string, string>[], gosomData: Record<string, string>[], deduplicationKeys: string[], prioritySource: string, gosomFields: string[], spiderFields: string[]): ConsolidatedData`
    - Esta funci√≥n debe:
        - Mapear campos (ej. si Spider usa 'telefono' y Gosom 'tel√©fono', o 'direccion' vs 'address').
        - Unir los dos conjuntos de datos.
        - Aplicar la l√≥gica de prioridad de `prioritySource` (Gosom) para campos en com√∫n si hay conflicto.
        - Deduplicar basado en `deduplicationKeys`.
    - Integrar esta funci√≥n en `ConsolidateTabContent.tsx`, usando los datos parseados del paso anterior y los par√°metros de `etl_params.json`.
    - Actualizar la UI para mostrar los datos consolidados reales y permitir su descarga como CSV.
4. **Implementar la l√≥gica real de chunking:**
    - En `src/lib/etl-logic.ts` (o similar), crear funci√≥n: `function generateChunks(consolidatedData: ConsolidatedData, chunkSize: number): DataChunk[]`.
    - Integrar en `ChunkingTabContent.tsx`, usando los datos consolidados reales.
    - Asegurar que la descarga de cada chunk funcione correctamente.
5. **Mejorar el feedback visual y manejo de estados (loading, success, error) en todas las pesta√±as durante estas operaciones.**

---

- **Carga a PostgreSQL: Migrar datos del CSV madre a la base de datos** (requiere Genkit flow o API de backend).


---

