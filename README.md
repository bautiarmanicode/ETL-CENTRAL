# ğŸš€âœ¨ ETL Central - EspecificaciÃ³n de la Interfaz y LÃ³gica de Procesamiento ğŸ“ŠğŸ¤–

---

## ğŸ¯ Objetivo General  
Centralizar el procesamiento de datos crudos de **Spider** y **GOSOM**, consolidÃ¡ndolos en un CSV Madre (conceptual, client-side) y generando chunks si es necesario. Este mÃ³dulo es implementado en Next.js con React y ShadCN UI.

---

## ğŸ“ Layout General  
La interfaz se divide en tres Ã¡reas principales:  

- ğŸ–¥ï¸ **Barra Lateral (Sidebar):**  
  - ConfiguraciÃ³n de parÃ¡metros (tamaÃ±o de chunk).  

- ğŸ“Š **Ãrea Principal (Main Area):**  
  - PestaÃ±as para:  
    - **Cargar CSVs**: Subir archivos desde Spider y Gosom.  
    - **Consolidar**: Merge con CSV Madre (simulado) + deduplicaciÃ³n (simulada).  
    - **Chunkear**: Dividir CSV Madre (simulado) en chunks.  
    - **Logs**: VisualizaciÃ³n de registros de ejecuciÃ³n.  

---

## ğŸƒâ€â™€ï¸ CÃ³mo Usar la AplicaciÃ³n

Para ejecutar la aplicaciÃ³n localmente, sigue estos pasos:

1.  AsegÃºrate de tener [Node.js](https://nodejs.org/en/) instalado en tu mÃ¡quina.
2.  Abre la terminal o lÃ­nea de comandos.
3.  Navega hasta el directorio raÃ­z de este proyecto donde se encuentra el archivo `package.json`.
4.  Instala las dependencias del proyecto ejecutando uno de los siguientes comandos (dependiendo del gestor de paquetes que prefieras):



## ğŸ› ï¸ Componentes y Funcionalidades Detalladas (Implementado con Next.js/React)

### 1. **Carga de CSVs Crudos**  
- **Formato Esperado (ValidaciÃ³n simulada):**  
  - **Spider:** `nombre`, `direcciÃ³n`, `telÃ©fono`.  
  - **Gosom:** `nombre`, `website`, `email`.  
- **UI (Conceptual, implementado con ShadCN `Input type="file"`):**  
  La aplicaciÃ³n (`src/app/page.tsx`) incluye componentes para cargar archivos CSV de Spider y Gosom, con validaciÃ³n simulada del tipo de archivo.

### 2. **ConsolidaciÃ³n y DeduplicaciÃ³n**  
- **LÃ³gica (Simulada en cliente):**  
  - Unir nuevos CSVs con datos consolidados en memoria.  
  - Deduplicar usando `link` y `title` (simulado). Los datos de Gosom tienen prioridad en caso de conflicto.

### 3. **Chunking (Opcional)**  
- **ParÃ¡metros:** TamaÃ±o de chunk (configurable en la UI).  
- **Salida:** Descarga de archivos CSV por chunk.

---

## ğŸ“‚ Ubicaciones de Archivos Clave (Conceptuales para el Flujo de Datos)
Dado que esta es una aplicaciÃ³n web frontend, los "archivos" y "directorios de datos" son manejados de la siguiente manera:
- **CSVs Crudos de Spider/Gosom:** Cargados a travÃ©s del navegador, procesados en memoria.
- **CSV Madre (Consolidado):** Estado en memoria dentro de la aplicaciÃ³n React. Puede ser descargado.
- **Chunks Generados:** Generados en memoria y disponibles para descarga individual.
- **Logs de EjecuciÃ³n:** Mantenidos en el estado de la aplicaciÃ³n y mostrados en la pestaÃ±a "Logs".

---

## ğŸ“Œ **Ejemplo de Uso**  
1. En la pestaÃ±a "Cargar CSVs", subir `leads_spider.csv` y `leads_gosom.csv`.  
2. Ir a la pestaÃ±a "Consolidar" y hacer clic en "Consolidar" para mergear y deduplicar (simulado).  
3. Si es necesario, ir a la pestaÃ±a "Chunkear", ajustar el tamaÃ±o de chunk, y generar chunks para descarga.  
4. Revisar la pestaÃ±a "Logs" para ver el historial de operaciones.

---

## ğŸ“š DocumentaciÃ³n Adicional  
- ğŸš€ **[Ideas Futuras](0_prompts/0_Futuro.md):**  
  - IntegraciÃ³n con backend (ej. PostgreSQL usando Genkit).  
  - AutomatizaciÃ³n de ejecuciones (requerirÃ­a un backend).  
- ğŸ“‹ **[Tareas Pendientes](0_prompts/1_Mejoras.md):**  
  - Mejorar la lÃ³gica real de deduplicaciÃ³n y chunking (actualmente simulada).  
- âœ… **[Historial de Tareas Completadas](0_prompts/2_Historial.md):**  
  - Registrar tareas resueltas.  

---

## ğŸ§  ReflexiÃ³n Final  
- **ETL Central (Data Refinery) es el cerebro del flujo del lado del cliente:** Spider y GOSOM son fuentes de datos CSVs crudos.  
- **Futuro en Backend:** Funcionalidades como la persistencia en PostgreSQL o la automatizaciÃ³n requerirÃ¡n un componente de backend.  
- **TecnologÃ­as:** Next.js, React, TypeScript, ShadCN UI, Tailwind CSS.  

---

*Â¡Gracias por leer esta documentaciÃ³n!  
ETL Central (Data Refinery) estÃ¡ diseÃ±ado para ser **modular y escalable dentro del paradigma frontend actual**, con potencial de expansiÃ³n a backend.* ğŸŒŸ  
